{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 512 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#small\n",
    "dataset_s = tf.keras.utils.image_dataset_from_directory(\"Dataset/\", batch_size=512, image_size=(120,120), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(list(dataset_s.as_numpy_iterator())[0][0])/255\n",
    "y = np.asarray(tf.one_hot(list(dataset_s.as_numpy_iterator())[0][1],depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.7,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small\n",
    "\n",
    "M1_s = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(48, 6, padding='same', strides=(4, 4), input_shape=(120, 120, 3)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, 5, padding='same'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(64, 'relu'),\n",
    "    tf.keras.layers.Dense(2, 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1_s.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "12/12 [==============================] - 7s 225ms/step - loss: 0.3740 - accuracy: 0.9218\n",
      "Epoch 2/8\n",
      " 6/12 [==============>...............] - ETA: 1s - loss: 0.0061 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m M1_s\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "M1_s.fit(x_train, y_train, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 98ms/step - loss: 0.0012 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0011675774585455656, 1.0]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1_s.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0) # registers cam object for your webcam\n",
    "key = '' # initializes key variable to contain no character.\n",
    "while key != 113: # checks if key equals 113 which is ascii for q\n",
    "    ret, frame = cam.read() # gets image from camera and stores it in frame\n",
    "    frame = cv2.resize(frame, (120, 120)) # resizes image to 240 x 240\n",
    "    infer = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # internally cv2 stores images as BGR\n",
    "    # this function changes it to rgb and stores it as infer\n",
    "    res = M1_s.predict(np.asarray(infer).reshape(1, 120, 120, 3)) # converts cv2 mat infer to numpy array and\n",
    "    # stores the result of a prediction as res\n",
    "    if res[0][1] > res[0][0]: # checks if activation for class b is higher than class a\n",
    "        cv2.rectangle(frame, (0, 0), (239, 239), (0, 255, 0), 5) # if activation is higher for b draws green edge on frame\n",
    "    else:\n",
    "        cv2.rectangle(frame, (0, 0), (239, 239), (0, 0, 255), 5) # if activation isn't higher for b draws red edge on frame\n",
    "    cv2.imshow(\"hello\", frame) # shows frame with either red or green edge.\n",
    "    key = cv2.waitKey(10) # waits 10 ms for keyboard input and stores the input in key, blank if no key pressed\n",
    "cam.release() # releases the webcam device\n",
    "cv2.destroyAllWindows() # destroys the named window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dset.pickle\", \"wb\") as f:\n",
    "    pickle.dump((x,y), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not search for non-variable resources. Concrete function internal representation may have changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 15:31:09.859143: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-10-28 15:31:09.859409: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-28 15:31:10.025591: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-10-28 15:31:10.025855: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "pass1 convert failed for name: \"sequential_5/conv2d_15/Conv2D\"\n",
      "op: \"Conv2D\"\n",
      "input: \"input\"\n",
      "input: \"sequential_5/conv2d_15/Conv2D/ReadVariableOp\"\n",
      "attr {\n",
      "  key: \"use_cudnn_on_gpu\"\n",
      "  value {\n",
      "    b: true\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"strides\"\n",
      "  value {\n",
      "    list {\n",
      "      i: 1\n",
      "      i: 4\n",
      "      i: 4\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"padding\"\n",
      "  value {\n",
      "    s: \"SAME\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"explicit_paddings\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dilations\"\n",
      "  value {\n",
      "    list {\n",
      "      i: 1\n",
      "      i: 1\n",
      "      i: 1\n",
      "      i: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"data_format\"\n",
      "  value {\n",
      "    s: \"NHWC\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      ", ex=Could not infer attribute `explicit_paddings` type from empty iterator\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not infer attribute `explicit_paddings` type from empty iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spec \u001b[39m=\u001b[39m (tf\u001b[39m.\u001b[39mTensorSpec((\u001b[39mNone\u001b[39;00m, \u001b[39m120\u001b[39m, \u001b[39m120\u001b[39m, \u001b[39m3\u001b[39m), tf\u001b[39m.\u001b[39mfloat32, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m),)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM1.onnx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/asif/Documents/GitHub/ApML/Lab3/Person_Detector.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_proto, _ \u001b[39m=\u001b[39m tf2onnx\u001b[39m.\u001b[39;49mconvert\u001b[39m.\u001b[39;49mfrom_keras(M1_s, input_signature\u001b[39m=\u001b[39;49mspec, opset\u001b[39m=\u001b[39;49m\u001b[39m13\u001b[39;49m, output_path\u001b[39m=\u001b[39;49moutput_path)\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/convert.py:502\u001b[0m, in \u001b[0;36mfrom_keras\u001b[0;34m(model, input_signature, opset, custom_ops, custom_op_handlers, custom_rewriter, inputs_as_nchw, outputs_as_nchw, extra_opset, shape_override, target, large_model, output_path, optimizers)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39m/cpu:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    500\u001b[0m     frozen_graph, initialized_tables \u001b[39m=\u001b[39m \\\n\u001b[1;32m    501\u001b[0m         tf_loader\u001b[39m.\u001b[39mfrom_trackable(model, concrete_func, input_names, output_names, large_model)\n\u001b[0;32m--> 502\u001b[0m     model_proto, external_tensor_storage \u001b[39m=\u001b[39m _convert_common(\n\u001b[1;32m    503\u001b[0m         frozen_graph,\n\u001b[1;32m    504\u001b[0m         name\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    505\u001b[0m         continue_on_error\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    506\u001b[0m         target\u001b[39m=\u001b[39;49mtarget,\n\u001b[1;32m    507\u001b[0m         opset\u001b[39m=\u001b[39;49mopset,\n\u001b[1;32m    508\u001b[0m         custom_ops\u001b[39m=\u001b[39;49mcustom_ops,\n\u001b[1;32m    509\u001b[0m         custom_op_handlers\u001b[39m=\u001b[39;49mcustom_op_handlers,\n\u001b[1;32m    510\u001b[0m         optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[1;32m    511\u001b[0m         custom_rewriter\u001b[39m=\u001b[39;49mcustom_rewriter,\n\u001b[1;32m    512\u001b[0m         extra_opset\u001b[39m=\u001b[39;49mextra_opset,\n\u001b[1;32m    513\u001b[0m         shape_override\u001b[39m=\u001b[39;49mshape_override,\n\u001b[1;32m    514\u001b[0m         input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m    515\u001b[0m         output_names\u001b[39m=\u001b[39;49moutput_names,\n\u001b[1;32m    516\u001b[0m         inputs_as_nchw\u001b[39m=\u001b[39;49minputs_as_nchw,\n\u001b[1;32m    517\u001b[0m         outputs_as_nchw\u001b[39m=\u001b[39;49moutputs_as_nchw,\n\u001b[1;32m    518\u001b[0m         large_model\u001b[39m=\u001b[39;49mlarge_model,\n\u001b[1;32m    519\u001b[0m         tensors_to_rename\u001b[39m=\u001b[39;49mtensors_to_rename,\n\u001b[1;32m    520\u001b[0m         initialized_tables\u001b[39m=\u001b[39;49minitialized_tables,\n\u001b[1;32m    521\u001b[0m         output_path\u001b[39m=\u001b[39;49moutput_path)\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m model_proto, external_tensor_storage\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/convert.py:168\u001b[0m, in \u001b[0;36m_convert_common\u001b[0;34m(frozen_graph, name, large_model, output_path, output_frozen_graph, custom_ops, custom_op_handlers, optimizers, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtflite_path\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtfjs_path\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    167\u001b[0m     tf\u001b[39m.\u001b[39mimport_graph_def(frozen_graph, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 168\u001b[0m g \u001b[39m=\u001b[39m process_tf_graph(tf_graph, const_node_values\u001b[39m=\u001b[39;49mconst_node_values,\n\u001b[1;32m    169\u001b[0m                      custom_op_handlers\u001b[39m=\u001b[39;49mcustom_op_handlers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m constants\u001b[39m.\u001b[39mENV_TF2ONNX_CATCH_ERRORS \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    171\u001b[0m     catch_errors \u001b[39m=\u001b[39m constants\u001b[39m.\u001b[39mENV_TF2ONNX_CATCH_ERRORS\u001b[39m.\u001b[39mupper() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTRUE\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/tfonnx.py:459\u001b[0m, in \u001b[0;36mprocess_tf_graph\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    456\u001b[0m     main_g, subgraphs \u001b[39m=\u001b[39m graphs_from_tfjs(tfjs_path, input_names, output_names, shape_override,\n\u001b[1;32m    457\u001b[0m                                          ignore_default, use_default)\n\u001b[1;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     main_g, subgraphs \u001b[39m=\u001b[39m graphs_from_tf(tf_graph, input_names, output_names, shape_override, const_node_values,\n\u001b[1;32m    460\u001b[0m                                        ignore_default, use_default)\n\u001b[1;32m    462\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m [main_g] \u001b[39m+\u001b[39m subgraphs:\n\u001b[1;32m    463\u001b[0m     g\u001b[39m.\u001b[39mset_config(target, opset, extra_opset)\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/tfonnx.py:474\u001b[0m, in \u001b[0;36mgraphs_from_tf\u001b[0;34m(tf_graph, input_names, output_names, shape_override, const_node_values, ignore_default, use_default)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m shape_override \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     shape_override \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 474\u001b[0m ordered_func \u001b[39m=\u001b[39m resolve_functions(tf_graph)\n\u001b[1;32m    475\u001b[0m subgraphs \u001b[39m=\u001b[39m []\n\u001b[1;32m    476\u001b[0m \u001b[39mfor\u001b[39;00m func \u001b[39min\u001b[39;00m ordered_func:\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/tf_loader.py:764\u001b[0m, in \u001b[0;36mresolve_functions\u001b[0;34m(tf_graph)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[39myield\u001b[39;00m ordered\n\u001b[1;32m    762\u001b[0m         data \u001b[39m=\u001b[39m {item: (dep \u001b[39m-\u001b[39m ordered) \u001b[39mfor\u001b[39;00m item, dep \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m item \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ordered}\n\u001b[0;32m--> 764\u001b[0m _, _, _, _, _, functions \u001b[39m=\u001b[39m tflist_to_onnx(tf_graph, {})\n\u001b[1;32m    765\u001b[0m data \u001b[39m=\u001b[39m {}\n\u001b[1;32m    766\u001b[0m \u001b[39mfor\u001b[39;00m k, fdef \u001b[39min\u001b[39;00m tf_graph\u001b[39m.\u001b[39m_functions\u001b[39m.\u001b[39mitems():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/tf2onnx/tf_utils.py:462\u001b[0m, in \u001b[0;36mtflist_to_onnx\u001b[0;34m(g, shape_override, const_node_values, ignore_default, use_default)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m takeit:\n\u001b[1;32m    461\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         onnx_node \u001b[39m=\u001b[39m helper\u001b[39m.\u001b[39;49mmake_node(node_type, input_names, output_names, name\u001b[39m=\u001b[39;49mnode\u001b[39m.\u001b[39;49mname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mattr)\n\u001b[1;32m    463\u001b[0m         onnx_nodes\u001b[39m.\u001b[39mappend(onnx_node)\n\u001b[1;32m    464\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/onnx/helper.py:164\u001b[0m, in \u001b[0;36mmake_node\u001b[0;34m(op_type, inputs, outputs, name, doc_string, domain, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     node\u001b[39m.\u001b[39mdomain \u001b[39m=\u001b[39m domain\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 164\u001b[0m     node\u001b[39m.\u001b[39mattribute\u001b[39m.\u001b[39mextend(\n\u001b[1;32m    165\u001b[0m         make_attribute(key, value)\n\u001b[1;32m    166\u001b[0m         \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(kwargs\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    167\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/onnx/helper.py:165\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m     node\u001b[39m.\u001b[39mdomain \u001b[39m=\u001b[39m domain\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m    164\u001b[0m     node\u001b[39m.\u001b[39mattribute\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 165\u001b[0m         make_attribute(key, value)\n\u001b[1;32m    166\u001b[0m         \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(kwargs\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    167\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Documents/GitHub/ApML/Lab3/.venv/lib/python3.11/site-packages/onnx/helper.py:876\u001b[0m, in \u001b[0;36mmake_attribute\u001b[0;34m(key, value, doc_string, attr_type)\u001b[0m\n\u001b[1;32m    874\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(value)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m attr_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    877\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not infer attribute `\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` type from empty iterator\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m     )\n\u001b[1;32m    879\u001b[0m \u001b[39mif\u001b[39;00m attr_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     types \u001b[39m=\u001b[39m {\u001b[39mtype\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value}\n",
      "\u001b[0;31mValueError\u001b[0m: Could not infer attribute `explicit_paddings` type from empty iterator"
     ]
    }
   ],
   "source": [
    "spec = (tf.TensorSpec((None, 120, 120, 3), tf.float32, name=\"input\"),)\n",
    "output_path = \"M1.onnx\"\n",
    "model_proto, _ = tf2onnx.convert.from_keras(M1_s, input_signature=spec, opset=13, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
